Data Parallel Inference for Large Language Models
A hassle-free data parallel code (ideal for inference) to load LLMS and run them on multiple GPUs concurrently without multiple class definitions.

Table of Contents
Overview
Requirements
Installation
Usage
License
Overview
This repository contains code that enables you to run inference using large language models (LLMs) on multiple GPUs concurrently. It simplifies the process of loading models and distributing tasks across available GPUs, making it ideal for parallel data processing during inference.

Requirements
Python 3.7+
PyTorch
Transformers library from Hugging Face
Installation
Clone the repository:
git clone https://github.com/yourusername/parallel-llm-inference.git
cd parallel-llm-inference
